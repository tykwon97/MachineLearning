{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week11_201814116_권태윤_spacing_rnn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CwxKv7Np5qMj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637761731423,"user_tz":-540,"elapsed":1353,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}},"outputId":"8b92ef5b-6bb1-4206-8a1c-4d02d1e50113"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"aY7_UjO06Emi","executionInfo":{"status":"ok","timestamp":1637761731426,"user_tz":-540,"elapsed":20,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}}},"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import (DataLoader, TensorDataset)\n","from sklearn.metrics import accuracy_score\n","\n","class SpacingRNN(nn.Module):\n","\n","    def __init__(self, config):\n","        super(SpacingRNN, self).__init__()\n","\n","        # 전체 음절 개수\n","        self.eumjeol_vocab_size = config[\"eumjeol_vocab_size\"]\n","\n","        # 음절 임베딩 사이즈\n","        self.embedding_size = config[\"embedding_size\"]\n","\n","        # RNN 히든 사이즈\n","        self.hidden_size = config[\"hidden_size\"]\n","\n","        # 분류할 라벨의 개수\n","        self.number_of_labels = config[\"number_of_labels\"]\n","\n","        # 임베딩층: 랜덤 초기화 후 fine-tuning\n","        # 이곳을 채우세요.\n","        self.embedding=nn.Embedding(num_embeddings=self.eumjeol_vocab_size, embedding_dim=self.embedding_size, padding_idx=0)\n","        #\n","\n","        self.dropout = nn.Dropout(config[\"dropout\"])\n","\n","        # RNN layer\n","        # 이곳을 채우세요.\n","        self.bi_lstm=nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n","        #\n","\n","        # fully_connected layer를 통하여 출력 크기를 number_of_labels에 맞춰줌\n","        # 이곳을 채우세요.\n","        \n","        self.linear1 = nn.Linear(self.hidden_size*2, self.hidden_size*4, bias=True) \n","        self.linear2 = nn.Linear(self.hidden_size*4, self.hidden_size*8, bias=True) \n","        self.linear3 = nn.Linear(in_features=self.hidden_size*8,out_features=self.number_of_labels)\n","\n","        #\n","\n","    def forward(self, inputs):\n","        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n","        eumjeol_inputs = self.embedding(inputs)\n","\n","        # hidden_outputs, hidden_states = self.bi_gru(eumjeol_inputs)\n","        hidden_outputs, hidden_states = self.bi_lstm(eumjeol_inputs)\n","\n","        # (batch_size, max_length, hidden_size*2)\n","        #이 부분 도 수정\n","        hidden_outputs1 = self.dropout(hidden_outputs)\n","\n","        hidden_outputs2 = self.linear1(hidden_outputs1)\n","\n","        hidden_outputs3 = self.dropout(hidden_outputs2)\n","\n","        hidden_outputs = self.linear2(hidden_outputs3)\n","        #\n","\n","        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_labels)\n","        hypothesis = self.linear3(hidden_outputs)\n","\n","        return hypothesis"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"unh9In2q6OQK","executionInfo":{"status":"ok","timestamp":1637761731428,"user_tz":-540,"elapsed":19,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}}},"source":["# 데이터를 읽어 리스트에 저장\n","def read_datas(file_path):\n","    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n","        lines = inFile.readlines()\n","    datas = []\n","    for line in lines:\n","        # 입력 문장을 \\t으로 분리\n","        pieces = line.strip().split(\"\\t\")\n","        # 입력 문자열을 음절 단위로 분리\n","        eumjeol_sequence, label_sequence = pieces[0].split(), pieces[1].split()\n","        datas.append((eumjeol_sequence, label_sequence))\n","    return datas\n","\n","# 데이터를 읽고 각각의 딕셔너리 생성\n","def read_vocab_data(eumjeol_vocab_data_path):\n","    label2idx, idx2label = {\"<PAD>\":0, \"B\":1, \"I\":2}, {0:\"<PAD>\", 1:\"B\", 2:\"I\"}\n","    eumjeol2idx, idx2eumjeol = {}, {}\n","\n","    with open(eumjeol_vocab_data_path, \"r\", encoding=\"utf8\") as inFile:\n","        lines = inFile.readlines()\n","\n","    for line in lines:\n","        eumjeol = line.strip()\n","        eumjeol2idx[eumjeol] = len(eumjeol2idx)\n","        idx2eumjeol[eumjeol2idx[eumjeol]] = eumjeol\n","\n","    return eumjeol2idx, idx2eumjeol, label2idx, idx2label\n","\n","def load_dataset(config):\n","    datas = read_datas(config[\"input_data\"])\n","    eumjeol2idx, idx2eumjeol, label2idx, idx2label = read_vocab_data(config[\"eumjeol_vocab\"])\n","\n","    # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터를 담을 리스트\n","    eumjeol_features, eumjeol_feature_lengths, label_features = [], [], []\n","\n","    for eumjeol_sequence, label_sequence in datas:\n","        eumjeol_feature = [eumjeol2idx[eumjeol] for eumjeol in eumjeol_sequence]\n","        label_feature = [label2idx[label] for label in label_sequence]\n","\n","        # 음절 sequence의 실제 길이\n","        eumjeol_feature_length = len(eumjeol_feature)\n","\n","        # 모든 입력 데이터를 고정된 길이로 맞춰주기 위한 padding 처리\n","        # 이곳을 채우세요.\n","        eumjeol_feature += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n","        label_feature += [0] * (config[\"max_length\"] - eumjeol_feature_length)\n","        #\n","\n","        # 변환한 데이터를 각 리스트에 저장\n","        eumjeol_features.append(eumjeol_feature)\n","        eumjeol_feature_lengths.append(eumjeol_feature_length)\n","        label_features.append(label_feature)\n","\n","    # 변환한 데이터를 Tensor 객체에 담아 반환\n","    eumjeol_features = torch.tensor(eumjeol_features, dtype=torch.long)\n","    eumjeol_feature_lengths = torch.tensor(eumjeol_feature_lengths, dtype=torch.long)\n","    label_features = torch.tensor(label_features, dtype=torch.long)\n","\n","    return eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBfpm6Ts6Reg","executionInfo":{"status":"ok","timestamp":1637761731967,"user_tz":-540,"elapsed":45,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}}},"source":["def train(config):\n","    # RNN 모델 객체 생성\n","    model = SpacingRNN(config).cuda()\n","\n","    # 데이터 읽기\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    # 학습 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n","    train_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    # 크로스엔트로피 비용 함수, padding은 계산하지 않음\n","    # 이곳을 채우세요.\n","    loss_func = nn.CrossEntropyLoss()\n","    #\n","\n","    # 모델 학습을 위한 optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(config[\"epoch\"]):\n","\n","        model.train()\n","        costs = []\n","\n","        for step, batch in enumerate(train_dataloader):\n","\n","            # 역전파 단계를 실행하기 전에 변화도를 0으로 변경\n","            optimizer.zero_grad()\n","\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터\n","            inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","            # 모델 출력 결과 얻어오기\n","            hypothesis = model(inputs)\n","\n","            # hypothesis : (batch_size, max_length, number_of_labels) -> (batch_size*max_length, number_of_labels)\n","            # labels : (batch_size, max_length) -> (batch_size*max_length, )\n","            # 이곳을 채우세요.\n","            cost = loss_func(hypothesis. reshape(-1,len(label2idx)), labels.flatten())\n","            #\n","\n","            cost.backward()\n","            optimizer.step()\n","\n","            # batch 단위 cost 값 저장\n","            costs.append(cost.data.item())\n","\n","        torch.save(model.state_dict(), os.path.join(output_dir, \"epoch_{0:d}.pt\".format(epoch + 1)))\n","\n","        # epoch 별로 평균 loss 값과 정확도 출력\n","        print(\"Average cost : {}\".format(np.mean(costs)))"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-A9SurF6UgG","executionInfo":{"status":"ok","timestamp":1637761731969,"user_tz":-540,"elapsed":43,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}}},"source":["# 모델 출력 라벨 sequence와 정답 라벨 sequence를 기반으로\n","# 모델 출력 문장과 정답 문장 출력\n","def make_sentence(inputs, predicts, labels, idx2eumjeol, idx2label):\n","\n","    predict_sentence, correct_sentence = \"\", \"\"\n","\n","    for index in range(len(inputs)):\n","        eumjeol = idx2eumjeol[inputs[index]]\n","        correct_label = idx2label[labels[index]]\n","        predict_label = idx2label[predicts[index]]\n","\n","        # 시작 음절인 경우 공백을 추가해줄 필요가 없음\n","        if (index == 0):\n","            predict_sentence += eumjeol\n","            correct_sentence += eumjeol\n","            continue\n","\n","        # \"B\" 태그인 경우 어절의 시작 음절이므로 앞에 공백을 추가\n","        if (predict_label == \"B\"):\n","            predict_sentence += \" \"\n","        predict_sentence += eumjeol\n","\n","        # \"B\" 태그인 경우 어절의 시작 음절이므로 앞에 공백을 추가\n","        if (correct_label == \"B\"):\n","            correct_sentence += \" \"\n","        correct_sentence += eumjeol\n","\n","    return predict_sentence, correct_sentence\n","\n","# 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","def test(config):\n","    # 데이터 읽기\n","    eumjeol_features, eumjeol_feature_lengths, label_features, eumjeol2idx, idx2eumjeol, label2idx, idx2label = load_dataset(config)\n","\n","    # 평가 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n","    test_features = TensorDataset(eumjeol_features, eumjeol_feature_lengths, label_features)\n","    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=1)\n","\n","    # RNN 모델 객체 생성\n","    model = SpacingRNN(config).cuda()\n","    # 사전학습한 모델 파일로부터 가중치 불러옴\n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"model_name\"])))\n","\n","    # 모델의 출력 결과와 실제 정답값을 담을 리스트\n","    total_hypothesis, total_labels = [], []\n","\n","    for step, batch in enumerate(test_dataloader):\n","\n","        model.eval()\n","        batch = tuple(t.cuda() for t in batch)\n","\n","        # 음절 데이터, 각 데이터의 실제 길이, 라벨 데이터\n","        inputs, input_lengths, labels = batch[0], batch[1], batch[2]\n","\n","        # 모델 평가\n","        hypothesis = model(inputs)\n","\n","        # (batch_size, max_length, number_of_labels) -> (batch_size, max_length)\n","        hypothesis = torch.argmax(hypothesis, dim=-1)\n","\n","        # batch_size가 1이기 때문\n","        input_length = tensor2list(input_lengths[0])\n","        input = tensor2list(inputs[0])[:input_length]\n","        label = tensor2list(labels[0])[:input_length]\n","        hypothesis = tensor2list(hypothesis[0])[:input_length]\n","\n","        # 출력 결과와 정답을 리스트에 저장\n","        total_hypothesis += hypothesis\n","        total_labels += label\n","\n","        if (step < 10):\n","            # 정답과 모델 출력 비교\n","            predict_sentence, correct_sentence = make_sentence(input, hypothesis, label, idx2eumjeol, idx2label)\n","            print(\"정답 : \" + correct_sentence)\n","            print(\"출력 : \" + predict_sentence)\n","            print()\n","\n","    # 정확도 출력\n","    print(\"Accuracy : {}\".format(accuracy_score(total_labels, total_hypothesis)))"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kj-JT2466U9u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637761847475,"user_tz":-540,"elapsed":115542,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}},"outputId":"c7c2247f-5fad-4f4a-da36-4f76740424dd"},"source":["if(__name__==\"__main__\"):\n","    root_dir = \"/gdrive/My Drive/colab/rnn/spacing\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(5),\n","              \"input_data\":os.path.join(root_dir, \"train.txt\"),\n","              \"output_dir_path\":output_dir,\n","              \"eumjeol_vocab\": os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n","              \"label_vocab\": os.path.join(root_dir, \"label_vocab.txt\"),\n","              \"eumjeol_vocab_size\": 2458,\n","              \"embedding_size\": 100,\n","              \"hidden_size\": 100,\n","              \"max_length\": 920,\n","              \"number_of_labels\": 3,\n","              \"epoch\":5,\n","              \"batch_size\":64,\n","              \"dropout\":0.3\n","              }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Average cost : 0.07743905031982856\n","Average cost : 0.017014262855901748\n","Average cost : 0.011932895053178072\n","Average cost : 0.009290652738624736\n","Average cost : 0.008093917577327052\n"]}]},{"cell_type":"code","metadata":{"id":"1BA8wtxg6Y9R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637762175337,"user_tz":-540,"elapsed":327915,"user":{"displayName":"권태윤","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12469158853173697691"}},"outputId":"9cc1db4d-7c6c-4bf6-e898-d55c8e71cfeb"},"source":["if(__name__==\"__main__\"):\n","    root_dir = \"/gdrive/My Drive/colab/rnn/spacing\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","              \"model_name\":\"epoch_{0:d}.pt\".format(5),\n","              \"input_data\":os.path.join(root_dir, \"train.txt\"),\n","              \"output_dir_path\":output_dir,\n","              \"eumjeol_vocab\": os.path.join(root_dir, \"eumjeol_vocab.txt\"),\n","              \"label_vocab\": os.path.join(root_dir, \"label_vocab.txt\"),\n","              \"eumjeol_vocab_size\": 2458,\n","              \"embedding_size\": 100,\n","              \"hidden_size\": 100,\n","              \"max_length\": 920,\n","              \"number_of_labels\": 3,\n","              \"epoch\":5,\n","              \"batch_size\":64,\n","              \"dropout\":0.3\n","              }\n","\n","    if(config[\"mode\"] == \"test\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["정답 : 부인이 정성들여 키운 진이 독살되었다는 것은 연박사에게도 매우 가슴아픈 일일 것이다.\n","출력 : 부인이 정성들여키운진이독살되었다는 것은 연박사에게도 매우가슴아픈 일 일것이다.\n","\n","정답 : 이같은 그의 가난은 어려서만이 아니라 자란 뒤에도 늙을 때까지 줄기차게 계속되었다.\n","출력 : 이 같은 그의 가난은 어려서만이 아니라자란 뒤에도 늙을 때까지 줄기차게 계속되었다.\n","\n","정답 : 어느 물리학자의 머리 속.\n","출력 : 어느 물리학자의 머리속.\n","\n","정답 : 말씨며 움직임이 세련되어 있었다.\n","출력 : 말 씨며 움직임이 세련되어 있었다.\n","\n","정답 : \"워싱턴과 뉴욕에서 차출한 인원까지 모두 12 명이네.\"\n","출력 : \"워싱턴과 뉴욕에서 차출한 인원까지 모두 12명이네.\"\n","\n","정답 : \"그 사람을 아직도 사랑하나요?\"\n","출력 : \"그 사람을 아직도 사랑하나요?\"\n","\n","정답 : \"나한테 걸린 건 행복한 거예요. 나를 훔치려는 사내들이 많아요. 무슨 말인지 알아요?\"\n","출력 : \"나한테 걸린 건 행복한 거예요. 나를 훔치려는 사내들이 많아요. 무슨 말인지 알아요?\"\n","\n","정답 : 김광민은 집에 돌아와 있었으나 그 얘기를 바로 주남 마을과 녹동 마을에서 일어난 일이었기 때문에 자세히 들을 수 있었다.\n","출력 : 김광민은 집에 돌아와 있었으나 그 얘기를 바로 주남마을 과 녹동마을에서 일어난 일이었기 때문에 자세히 들을 수 있었다.\n","\n","정답 : 미리 이야기해 두는 게 좋을 것 같아.\"\n","출력 : 미리이야기해 두는 게 좋을 것 같아.\"\n","\n","정답 : \"부인의 입장에서 남편의 자살을 얘기하는 말투로는 너무 냉정하다는 생각 안 드세요?\"\n","출력 : \"부인의 입장에서 남편의 자살을 얘기하는 말투로는 너무 냉정하다는 생각안드세요?\"\n","\n","Accuracy : 0.9236905515260673\n"]}]}]}